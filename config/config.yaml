nmp_config:
  nemo_base_url: "http://nemo.test"
  nim_base_url: "http://nim.test"
  datastore_base_url: "http://data-store.test"


  # All resources created in NeMo Microservices Platform (NMP) will be namespaced to this value
  nmp_namespace: "dfwbp"

# Logging configuration
logging_config:
  level: "INFO"  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL

# MLflow configuration
# Note: MLflow is controlled by COMPOSE_PROFILES environment variable
# Set COMPOSE_PROFILES=mlflow to enable both the MLflow service and configuration
mlflow_config:
  # enabled: automatically set based on COMPOSE_PROFILES environment variable
  tracking_uri: "http://0.0.0.0:5000"
  experiment_name_prefix: "data-flywheel"
  artifact_location: "./mlruns"

llm_judge_config:
  deployment_type: "remote"
  url: "https://integrate.api.nvidia.com/v1/chat/completions"
  model_name: "meta/llama-3.1-70b-instruct"

nims:
  - model_name: "meta/llama-3.2-1b-instruct"
    model_type: "llm"
    context_length: 8192
    gpus: 1
    pvc_size: 25Gi
    tag: "1.8.3"
    customization_enabled: true
    customizer_configs:
      target: "meta/llama-3.2-1b-instruct@2.0"
      gpus: 1
      max_seq_length: 8192


  - model_name: "meta/llama-3.2-3b-instruct"
    model_type: "llm"
    context_length: 8192
    gpus: 1
    pvc_size: 25Gi
    tag: "1.8.3"
    customization_enabled: true
    customizer_configs:
      target: "meta/llama-3.2-3b-instruct@2.0"
      gpus: 1
      max_seq_length: 8192

  - model_name: "meta/llama-3.1-8b-instruct"
    model_type: "llm"
    context_length: 8192
    gpus: 1
    pvc_size: 25Gi
    tag: "1.8.3"
    customization_enabled: true
    customizer_configs:
      target: "meta/llama-3.1-8b-instruct@2.0"
      gpus: 1
      max_seq_length: 8192
# Data split config:
# train, val, eval split sizes and ratios
data_split_config:
  eval_size: 100
  val_ratio: 0.1
  min_total_records: 50
  random_seed: 42
  limit: null  # null = use all available records after filtering
  parse_function_arguments: true # parse function arguments to JSON objects for tool calling records
  stratify_enabled: true # Enable stratified splitting to maintain class balance
  min_samples_per_class: 2 # Minimum samples required per class for stratification
  rare_class_threshold: 1 # Group classes with <= this many samples as 'others'

# Evaluation configuration
evaluation_config:
  # Workload type: "auto" (auto-detect), "classification", or "tool_calling"
  # - auto: Automatically detect based on presence of tool_calls in data
  # - classification: Force use of classification evaluation (F1 score for chat completion)
  # - tool_calling: Force use of tool calling evaluation (function name accuracy, etc.)
  workload_type: "classification"
  
  # For tool_calling workloads, specify which evaluation type to use:
  # - tool-calling-metric: Use exact match metrics (function name, args)
  # - tool-calling-judge: Use LLM-as-judge for correctness (requires llm_judge_config)
  tool_eval_type: "tool-calling-metric"

# Training config:
# Customization config with default values
training_config:
  training_type: "sft"
  finetuning_type: "lora"
  epochs: 2
  batch_size: 128
  learning_rate: 0.0001

# LoRA config:
# adapter dimension, adapter dropout, sequence packing
lora_config:
  adapter_dim: 16
  adapter_dropout: 0.1
  sequence_packing_enabled: true  # Enable sequence packing for efficient training (boolean, not string!)
